{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy5vc2NoaW5hLm5ldC91cGxvYWRzL3NwYWNlLzIwMTUvMTIxNC8xOTAwMTZfemo3Wl8yMzE1MjQ3LnBuZw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键字怎么提取的，TF-IDF有改进么，怎么改进的\n",
    "\n",
    "TF（Term Frequency）词频，在文章中出现次数最多的词，然而文章中出现次数较多的词并不一定就是关键词，比如常见的对文章本身并没有多大意义的停用词。所以我们需要一个重要性调整系数来衡量一个词是不是常见词。该权重为IDF（Inverse Document Frequency）逆文档频率，它的大小与一个词的常见程度成反比。在我们得到词频（TF）和逆文档频率（IDF）以后，将两个值相乘，即可得到一个词的TF-IDF值，某个词对文章的重要性越高，其TF-IDF值就越大，所以排在最前面的几个词就是文章的关键词。\n",
    "\n",
    "TF-IDF的不足\n",
    "1、没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。\n",
    "\n",
    "2、按照传统TF-IDF函数标准，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。(换句话说，如果一个特征项只在某一个类别中的个别文本中大量出现，在类内的其他大部分文本中出现的很少，那么不排除这些个别文本是这个类中的特例情况，因此这样的特征项不具有代表性。)\n",
    "\n",
    "3、传统TF-IDF函数中的IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况。\n",
    "\n",
    "4、对于文档中出现次数较少的重要人名、地名信息提取效果不佳。\n",
    "\n",
    "TF部分的改进\n",
    "这里考虑将文档内的词频率更改为同一类文档内的词频率可以在一定程度上解决上面提到的第2项不足之处。\n",
    "IDF部分的改进\n",
    "\n",
    "传统的IDF通常可以写作：IDF=log(总文档数N/所有含特征词文档数n+0.01)\n",
    "\n",
    "在我查阅的所有论文中都提到了上面的第3项不足，这是TF-IDF应用于分类问题上的一个很明显的不足，针对这个不足，这些论文中也提到了不同的解决方法：\n",
    "\n",
    "①IDF=log(本类含特征词文档数m*总文档数N/所有含特征词文档数n+0.01)\n",
    "\n",
    "②用P（Mk）表示特征词Mk在当前类别中的频率，P（Mk）’表示特征词Mk在其他类别中的频率，对IDF计算改进如下\n",
    "\n",
    "$$IDF = log(1+\\frac{P(m_k)}{P(m_k)+{P(m_k)}'})$$\n",
    "\n",
    "![!image](https://img-blog.csdn.net/20150303100444956?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZnlmbWZvZg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost的原理\n",
    "XGBoost（eXtreme Gradient Boosting）全名叫极端梯度提升，XGBoost是集成学习方法的王牌，在Kaggle数据挖掘比赛中，大部分获胜者用了XGBoost，XGBoost在绝大多数的回归和分类问题上表现的十分顶尖，本文较详细的介绍了XGBoost的算法原理。\n",
    "\n",
    "CART － Classification and Regression Trees\n",
    "\n",
    "既然xgboost就是一个监督模型，那么我们的第一个问题就是：xgboost对应的模型是什么？\n",
    "答案就是一堆CART树。\n",
    "此时，可能我们又有疑问了，CART树是什么？这个问题请查阅其他资料，我的博客中也有相关文章涉及过。然后，一堆树如何做预测呢？答案非常简单，就是将每棵树的预测值加到一起作为最终的预测值，可谓简单粗暴。\n",
    "\n",
    "下图就是CART树和一堆CART树的示例，用来判断一个人是否会喜欢计算机游戏：\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/1371984-a90c565a27c9874d.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/835/format/webp)\n",
    "![](https://upload-images.jianshu.io/upload_images/1371984-bbe17b3b253a6d1a.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/901/format/webp)\n",
    "\n",
    "xgboost为什么使用CART树而不是用普通的决策树呢？\n",
    "简单讲，对于分类问题，由于CART树的叶子节点对应的值是一个实际的分数，而非一个确定的类别，这将有利于实现高效的优化算法。xgboost出名的原因一是准，二是快，之所以快，其中就有选用CART树的一份功劳。\n",
    "\n",
    "ID3、C4.5、CART的区别\n",
    "这三个是非常著名的决策树算法。简单粗暴来说，ID3 使用信息增益作为选择特征的准则；C4.5 使用信息增益比作为选择特征的准则；CART 使用 Gini 指数作为选择特征的准则。\n",
    "\n",
    "一、ID3\n",
    "熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。\n",
    "\n",
    "信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性 a 来进行划分所获得的 “纯度提升” 越大 **。也就是说，用属性 a 来划分训练集，得到的结果中纯度比较高。\n",
    "\n",
    "ID3 仅仅适用于二分类问题。ID3 仅仅能够处理离散属性。\n",
    "\n",
    "二、C4.5\n",
    "\n",
    "C4.5 克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 / 划分前熵 选择信息增益比最大的作为最优特征。\n",
    "\n",
    "C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。\n",
    "\n",
    "三、CART\n",
    "\n",
    "CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。\n",
    "\n",
    "CART 的全称是分类与回归树。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。\n",
    "\n",
    "回归树中，使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。\n",
    "\n",
    "要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。\n",
    "\n",
    "分类树种，使用 Gini 指数最小化准则来选择特征并进行划分；\n",
    "\n",
    "Gini 指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。\n",
    "\n",
    "信息增益 vs 信息增益比\n",
    "\n",
    "之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。\n",
    "\n",
    "Gini 指数 vs 熵\n",
    "\n",
    "既然这两个都可以表示数据的不确定性，不纯度。那么这两个有什么区别那？\n",
    "\n",
    "Gini 指数的计算不需要对数运算，更加高效；\n",
    "Gini 指数更偏向于连续属性，熵更偏向于离散属性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM原始问题为什么要转化为对偶问题，为什么对偶问题就好求解，原始问题不能求解么\n",
    "- K-means 中我想聚成100类 结果发现只能聚成98类，为什么\n",
    "- 进程中的内存分段是怎样的\n",
    "- GBDT的原理，以及常用的调参的参数\n",
    "- xgboost的跟GBDT比优点都有哪些\n",
    "- L1、L2正则化，区别\n",
    "- Xgboost中的行抽样，可以起到哪些作用\n",
    "- 样本少了不是会过拟合么，为什么行抽样可以防止过拟合\n",
    "- 常用的损失函数和适用场景\n",
    "- LR和SVM原理\n",
    "- LR和SVM这两个应用起来有什么不同\n",
    "- PCA说一下\n",
    "- 你都会什么聚类方法\n",
    "- 模型的评价方法有哪些\n",
    "- ROC怎么画\n",
    "- 你知道SoftMax么\n",
    "- 特征选择方法都有用过哪些\n",
    "- 随机森林怎么进行特征选择\n",
    "- 用过哪些机器学习算法\n",
    "- 加密方法知道哪些\n",
    "- MD5可逆么\n",
    "- word2vec用过么\n",
    "- 极大似然估计是什么意思\n",
    "- 说一下随机森林和Adaboost，以及区别\n",
    "- 说一下GBDT和Adaboost，以及区别\n",
    "- 说一下LDA的原理\n",
    "- 对于PCA，会有第一主成分、第二主成分，怎么为什么第一主成分是第一，原因是什么？\n",
    "- PCA的主成分是怎么得到的\n",
    "- 说一下SVM\n",
    "- 面向对象的三要素\n",
    "- 对深度学习了解多少\n",
    "- 你觉得深度学习的方法和传统机器学习比，有什么大的优势\n",
    "- 当我们要求准确率很高，但是不在意召回率的时候，可以怎样处理。\n",
    "- 回归算法用于分类的阈值如何确定呢\n",
    "- xgboost，说一下原理，步长如何设定\n",
    "- k-means中的k如何确定呢？\n",
    "- 除了k-means，还可以用什么聚类方法，或者你还熟悉什么聚类方法\n",
    "- 层次聚类的话，你又如何判断聚成多少类合适？\n",
    "- 朴素贝叶斯原理\n",
    "- TF-IDF原理\n",
    "- 性能评价指标，准确率召回率是怎么回事，二分类 和多分类的评价方法\n",
    "- LDA你是怎么用的，LDA的表现如何，主题分的效果好不好\n",
    "- 你觉得基于内容的方法和协同过滤有什么不同\n",
    "- 常用的推荐算法都有什么\n",
    "- 集成学习为什么要用简单的基学习器，不用一个复杂一点的学习器\n",
    "- 非线性的数据，可以使用什么分类器进行分类\n",
    "- LDA的原理是什么？\n",
    "- 推荐的时候矩阵一定是稀疏的，对于这个稀疏矩阵应该如何处理？\n",
    "- 如何从文档中提取关键字？\n",
    "- 讲一讲tf-idf是什么意思\n",
    "- hashmap你用过么，底层是如何实现的？\n",
    "- 手撸代码，不用库函数求一个数的立方根，要求误差小于0.01\n",
    "\n",
    "\n",
    "## 专业知识归纳"
   ]
  }
 ]
}