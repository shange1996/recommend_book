{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved by word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进思路：1\n",
    "## 加入停用词，blurb中的简介利用word2vec训练的向量替代，效果比原版提升了10%， 在15代的时候达到最好。loss=10.7, val_loss=11.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进思路：2\n",
    "## 改变模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)\n",
      "tensorflow 2.0.0\n",
      "matplotlib 3.1.1\n",
      "numpy 1.16.4\n",
      "pandas 0.25.3\n",
      "sklearn 0.22.1\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Blurb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14282</td>\n",
       "      <td>[6675, 4061, 1145]</td>\n",
       "      <td>15841</td>\n",
       "      <td>[3690, 190, 21279]</td>\n",
       "      <td>3482</td>\n",
       "      <td>64</td>\n",
       "      <td>1164</td>\n",
       "      <td>[35433, 23282, 61745, 85975, 79685, 556, 12159...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14282</td>\n",
       "      <td>[6675, 4061, 1145]</td>\n",
       "      <td>11233</td>\n",
       "      <td>[4686, 19363, 17005, 16035, 23204, 19399, 2235...</td>\n",
       "      <td>1412</td>\n",
       "      <td>72</td>\n",
       "      <td>1885</td>\n",
       "      <td>[51704, 47741, 55217, 42148, 106417, 61745, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14922</td>\n",
       "      <td>[6382, 5068, 1145]</td>\n",
       "      <td>11233</td>\n",
       "      <td>[4686, 19363, 17005, 16035, 23204, 19399, 2235...</td>\n",
       "      <td>1412</td>\n",
       "      <td>72</td>\n",
       "      <td>1885</td>\n",
       "      <td>[51704, 47741, 55217, 42148, 106417, 61745, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11046</td>\n",
       "      <td>[6090, 5109, 2622]</td>\n",
       "      <td>11233</td>\n",
       "      <td>[4686, 19363, 17005, 16035, 23204, 19399, 2235...</td>\n",
       "      <td>1412</td>\n",
       "      <td>72</td>\n",
       "      <td>1885</td>\n",
       "      <td>[51704, 47741, 55217, 42148, 106417, 61745, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1781</td>\n",
       "      <td>[3435, 4524, 2622]</td>\n",
       "      <td>11233</td>\n",
       "      <td>[4686, 19363, 17005, 16035, 23204, 19399, 2235...</td>\n",
       "      <td>1412</td>\n",
       "      <td>72</td>\n",
       "      <td>1885</td>\n",
       "      <td>[51704, 47741, 55217, 42148, 106417, 61745, 10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID            Location   ISBN  \\\n",
       "0    14282  [6675, 4061, 1145]  15841   \n",
       "1    14282  [6675, 4061, 1145]  11233   \n",
       "2    14922  [6382, 5068, 1145]  11233   \n",
       "3    11046  [6090, 5109, 2622]  11233   \n",
       "4     1781  [3435, 4524, 2622]  11233   \n",
       "\n",
       "                                               Title  Author  Year  Publisher  \\\n",
       "0                                 [3690, 190, 21279]    3482    64       1164   \n",
       "1  [4686, 19363, 17005, 16035, 23204, 19399, 2235...    1412    72       1885   \n",
       "2  [4686, 19363, 17005, 16035, 23204, 19399, 2235...    1412    72       1885   \n",
       "3  [4686, 19363, 17005, 16035, 23204, 19399, 2235...    1412    72       1885   \n",
       "4  [4686, 19363, 17005, 16035, 23204, 19399, 2235...    1412    72       1885   \n",
       "\n",
       "                                               Blurb  \n",
       "0  [35433, 23282, 61745, 85975, 79685, 556, 12159...  \n",
       "1  [51704, 47741, 55217, 42148, 106417, 61745, 10...  \n",
       "2  [51704, 47741, 55217, 42148, 106417, 61745, 10...  \n",
       "3  [51704, 47741, 55217, 42148, 106417, 61745, 10...  \n",
       "4  [51704, 47741, 55217, 42148, 106417, 61745, 10...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORIGIN_DATA_DIR = os.getcwd()+'/all_fearures/BX-CSV-Dump/'\n",
    "FILTERED_DATA_DIR = os.getcwd()+'/tmp/'\n",
    "class DataLoad:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        books_with_blurbs.csv cloumns: ISBN,text,Author,Year,Publisher,Blurb\n",
    "        BX-Book-Ratings.csv cloumns: User-ID,ISBN,Book-Rating\n",
    "        BX-Books.csv cloumns: ISBN,Book-text,Book-Author,Year-Of-Publication,Publisher,Image-URL-S,Image-URL-M,Image-URL-L\n",
    "        BX-Users.csv cloumns: User-ID,Location,Age\n",
    "        '''\n",
    "        self.BX_Users = self.load_origin('BX-Users')\n",
    "        self.BX_Book_Ratings = self.load_origin('BX-Book-Ratings')\n",
    "        self.Books = self.load_origin('books_with_blurbs', ',')\n",
    "        #合并三个表\n",
    "        self.features, self.ISBN2int, self.UserID2int, self.Users, self.blurb2int  = self.get_features()\n",
    "        self.labels = self.features.pop('Book-Rating')\n",
    "\n",
    "    def load_origin(self, \n",
    "        filename: \"根据文件名获取源文件，获取正确得columns、values等值\", \n",
    "        sep: \"因为源文件的分隔方式sep不同，所以通过传参改编分隔方式\"=\"\\\";\\\"\", \n",
    "        )->pd.DataFrame:\n",
    "        '''\n",
    "        获取原始数据，第一遍获取后将用pickle保存到本地，方便日后调用\n",
    "        '''\n",
    "        try:\n",
    "            # 从缓存的文件夹FILTERED_DATA_DIR获取基本被过滤后的文件\n",
    "            pickled_data = pickle.load(open(FILTERED_DATA_DIR+filename+'.p', mode='rb'))\n",
    "            return pickled_data\n",
    "        except FileNotFoundError:\n",
    "            # 如果缓存的文件不存在或者没有，则在源目录ORIGIN_DATA_DIR获取\n",
    "            all_fearures = pd.read_csv(ORIGIN_DATA_DIR+filename+'.csv', engine='python',sep=sep, encoding='utf-8')\n",
    "            # \\\";\\\"  初始过滤的文件\n",
    "            # ,      初始不需要过滤的文件\n",
    "            data_dict = {\"\\\";\\\"\":self.filtrator(all_fearures), ',':all_fearures}\n",
    "            # 因为没获得处理后的文件，所以我们在获取源文件后可以保存一下处理后的文件\n",
    "            pickle.dump((data_dict[sep]), open(FILTERED_DATA_DIR+filename+'.p', 'wb'))\n",
    "            return data_dict[sep]\n",
    "        except UnicodeDecodeError as e:\n",
    "            ''' 测试时经常会出现编码错误，如果尝试更换编码方式无效，可以将编码错误的部分位置重新复制粘贴就可以了，这里我们都默认UTF-8'''\n",
    "            print('UnicodeDecodeError:',e)\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(\"connect error|pandas Error: %s\" % e)\n",
    "\n",
    "    def filtrator(self, \n",
    "        f_data: \"输入需要进行初步filter的数据\"\n",
    "        )->pd.DataFrame:\n",
    "        '''\n",
    "        源文件中的columns和各个值得第一列的第一个字符和最后一列的最后一个字符都带有双引号‘\"’,需要将其filter,Location字段当用户Age为null的时候，末尾会有\\\";NULL字符串 ，直接用切片调整\n",
    "        '''\n",
    "        Nonetype_age = 0\n",
    "        f_data = f_data.rename(columns={f_data.columns[0]:f_data.columns[0][1:], f_data.columns[-1]:f_data.columns[-1][:-1]})\n",
    "        f_data[f_data.columns[0]] = f_data[f_data.columns[0]].map(lambda v:v[1:] if v!=None else Nonetype_age)\n",
    "        f_data[f_data.columns[-1]] = f_data[f_data.columns[-1]].map(lambda v:v[:-1] if v!=None else Nonetype_age)\n",
    "        try:\n",
    "            f_data = f_data[f_data['Location'].notnull()][f_data[f_data['Location'].notnull()]['Location'].str.contains('\\\";NULL')]\n",
    "            f_data['Location'] = f_data['Location'].map(lambda location:location[:-6])\n",
    "        except:\n",
    "            pass\n",
    "        return f_data\n",
    "\n",
    "    def get_features(self):\n",
    "        '''\n",
    "        获取整个数据集的所有features，并对每个文本字段作xxxxx\n",
    "        User-ID、Location、ISBN、Book-Rating、Title、Author、Year、Publisher、Blurb\n",
    "        '''\n",
    "        try:\n",
    "            # 从缓存的文件夹FILTERED_DATA_DIR获取features的文件\n",
    "            all_fearures, ISBN2int, UserID2int, Users, blurb2int = pickle.load(open(FILTERED_DATA_DIR+'features.p', mode='rb'))\n",
    "            return all_fearures, ISBN2int, UserID2int, Users\n",
    "        except:\n",
    "            # 将所有的数据组成features大表\n",
    "            all_fearures = pd.merge(pd.merge(self.BX_Users, self.BX_Book_Ratings), self.Books)\n",
    "            Users = all_fearures\n",
    "            # 因为没获得处理后的文件，所以我们在获取源文件后可以保存一下处理后的文件\n",
    "            # isbn2index userid2index\n",
    "            all_fearures.pop('Age')\n",
    "            all_fearures['Title'], title2int = self.feature2int(all_fearures['Title'], 'text')\n",
    "            all_fearures['Blurb'], blurb2int  = self.feature2int(all_fearures['Blurb'], 'text')\n",
    "            all_fearures['ISBN'], ISBN2int = self.feature2int(all_fearures['ISBN'], 'word')\n",
    "            all_fearures['Author'], X2int = self.feature2int(all_fearures['Author'], 'word')\n",
    "            all_fearures['Publisher'], X2int = self.feature2int(all_fearures['Publisher'], 'word')\n",
    "            all_fearures['Year'], X2int = self.feature2int(all_fearures['Year'], 'word')\n",
    "            all_fearures['User-ID'], UserID2int  = self.feature2int(all_fearures['User-ID'], 'word')\n",
    "            all_fearures['Location'] = self.feature2int(all_fearures['Location'], 'list')\n",
    "            all_fearures['Book-Rating'] = all_fearures['Book-Rating'].astype('float32')\n",
    "            pickle.dump((all_fearures, ISBN2int, UserID2int, Users), open(FILTERED_DATA_DIR+'features.p', 'wb'))\n",
    "            return all_fearures, ISBN2int, UserID2int, Users, blurb2int\n",
    "\n",
    "    def feature2int(self, \n",
    "        feature:'特征值',\n",
    "        feature_type:'text/word/list'):\n",
    "        '''\n",
    "        将文本字段比如title、blurb只取英文单词，并用空格为分隔符，做成一个带index值的集合，并用index值表示各个单词，作为文本得表示\n",
    "        '''\n",
    "        pattern = re.compile(r'[^a-zA-Z]')\n",
    "        filtered_map = {val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) }\n",
    "        letter_filter = lambda feature:feature.map({val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) })\n",
    "        text_words = set()\n",
    "        filtered_feature = letter_filter(feature)\n",
    "        for val in filtered_feature.str.split():\n",
    "            text_words.update(val)\n",
    "        text2int = {val:ii for ii, val in enumerate(text_words)}\n",
    "        text_map = {val:[text2int[row] for row in filtered_map[val].split()][:200] for ii,val in enumerate(set(feature))}\n",
    "              \n",
    "        word_map = {val:ii for ii,val in enumerate(set(feature))}\n",
    "        try:\n",
    "            cities = set()\n",
    "            for val in feature.str.split(','):\n",
    "                cities.update(val)\n",
    "            city2int = {val:ii for ii, val in enumerate(cities)}\n",
    "            list_map = {val:[city2int[row] for row in val.split(',')][:3] for ii,val in enumerate(set(feature))}\n",
    "        except AttributeError :\n",
    "            list_map = {}\n",
    "\n",
    "        feature_dict = {\n",
    "            'text':(feature.map(text_map), text_map),\n",
    "            'word':(feature.map(word_map), word_map),\n",
    "            'list':(feature.map(list_map)),\n",
    "            }\n",
    "        return feature_dict[feature_type]\n",
    "\n",
    "    def __del__(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "origin_DATA = DataLoad()\n",
    "origin_DATA.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepoces_blurb():\n",
    "    blurb_word2int = origin_DATA.blurb2int\n",
    "    blurb_int2word = {tuple(v): i for i,v in blurb_word2int.items()}\n",
    "    # 在这里我拿到了所有的句子\n",
    "    blurb_word_list = [blurb_int2word[tuple(val)] for val in origin_DATA.features.Blurb]\n",
    "    # 过滤停用词\n",
    "    stop_words = stopwords.words('english')\n",
    "    for sw in [',', ', ', ' ,', '«', '»', 'même', 'à', 'orée', '…', 'l', '', ' ', '``', '#']:\n",
    "        stop_words.append(sw)\n",
    "    new_sentences = []\n",
    "    for sentences in blurb_word_list:\n",
    "        sen_list = nltk.word_tokenize(sentences)\n",
    "        new_wrod_list = []\n",
    "        for word_ in sen_list:\n",
    "            if word_ not in stop_words:\n",
    "                new_wrod_list.append(word_)\n",
    "        new_sentences.append(new_wrod_list)\n",
    "    with open(\"./blurb.txt\", \"w\", encoding='utf-8') as f:\n",
    "        for sen in new_sentences:\n",
    "            for word_ in sen:\n",
    "                f.write(word_)\n",
    "                f.write(' ')\n",
    "            f.write('\\n')\n",
    "if not os.path.exists('./blurb.txt'):\n",
    "    prepoces_blurb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_blurb(blurb_path='./blurb.txt'):\n",
    "    blurb_new = pd.read_csv(blurb_path, names=['Blurb'], sep='\\t')\n",
    "    blurb_word_list = []\n",
    "    blurb_set = set()\n",
    "    for sen in blurb_new.values:\n",
    "        sen_list = nltk.word_tokenize(sen[0])\n",
    "        new_word_list_ = []\n",
    "        # 对于某些异常的句子，应该将其处理掉\n",
    "        if len(sen_list) > 600:\n",
    "            sen_list = sen_list[:30]\n",
    "        for word_ in sen_list:\n",
    "            new_word_list_.append(word_)\n",
    "            blurb_set.add(word_)\n",
    "        blurb_word_list.append(new_word_list_)\n",
    "    # 建立字典\n",
    "    blurb_dict = {v:i for i, v in enumerate(blurb_set)}\n",
    "    return blurb_word_list, blurb_dict\n",
    "\n",
    "blurb_word_list, blurb_dict = cut_blurb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurb_embedding_dim = 50\n",
    "def trainWord2Vec(size=blurb_embedding_dim):#训练word2vec模型并存储\n",
    "    train_sentences = blurb_word_list\n",
    "    # 这里min_count会对字典做截断，如果min_coun=5,那么出现次数小于5的词会被丢弃，需要更新原本的字典\n",
    "    model=Word2Vec(sentences=train_sentences,size=size,min_count=0,window=5)\n",
    "    model.save('./blurb_word2vec.model')\n",
    "trainWord2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159655, 50)\n",
      "(159656, 50)\n"
     ]
    }
   ],
   "source": [
    "def load_word2vec_martics(path = './blurb_word2vec.model', size=blurb_embedding_dim):\n",
    "    # 得到向量矩阵\n",
    "    model=Word2Vec.load(path)\n",
    "    blurb_word2vec_martics = np.zeros((len(blurb_dict), size))\n",
    "    for i, value in blurb_dict.items():\n",
    "        try:\n",
    "            blurb_word2vec_martics[value, :] = model.wv[i]\n",
    "        except:\n",
    "            print(i ,value)\n",
    "            print(model.wv[i])\n",
    "            exit(1)\n",
    "    print(blurb_word2vec_martics.shape)\n",
    "    # 添加pad字符对应的向量\n",
    "    blurb_word2vec_martics = np.insert(blurb_word2vec_martics, blurb_word2vec_martics.shape[0], axis=0, values=np.zeros(shape=(1, size)))\n",
    "    print(blurb_word2vec_martics.shape)\n",
    "    return blurb_word2vec_martics\n",
    "\n",
    "blurb_word2vec_martics = load_word2vec_martics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172102\n",
      "172102\n"
     ]
    }
   ],
   "source": [
    "# 增加一个pad字符\n",
    "print(len(blurb_word_list))\n",
    "blurb_dict['<padd>'] = len(blurb_dict)\n",
    "print(len(blurb_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_index =  159655\n",
      "(172102, 200)\n"
     ]
    }
   ],
   "source": [
    "def pre_blurb():\n",
    "    # 将blurb转换为数字矩阵，并处理长度\n",
    "    pad_index = blurb_dict['<padd>']\n",
    "    print('pad_index = ', pad_index)\n",
    "    blurb_int_list = []\n",
    "    for one_blurb in blurb_word_list:\n",
    "        # 补齐和去长\n",
    "        one_blurb = one_blurb[:200]\n",
    "        if len(one_blurb)< 200:\n",
    "            one_blurb = one_blurb + ['<padd>'] * (200- len(one_blurb))\n",
    "\n",
    "        # 转化为数字列表\n",
    "        temp_list = []\n",
    "        for word in one_blurb:\n",
    "            temp_list.append(blurb_dict[word])\n",
    "        blurb_int_list.append(temp_list)\n",
    "\n",
    "    blurb = np.array(blurb_int_list)\n",
    "    print(blurb.shape)\n",
    "    return blurb\n",
    "\n",
    "blurb = pre_blurb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all user id =  28836\n",
      "all location =  7573\n"
     ]
    }
   ],
   "source": [
    "# user-id的字典,总共有28836个用户\n",
    "all_user = len(set(origin_DATA.features['User-ID']))\n",
    "new_user_id = {val: i for i, val in enumerate(set(origin_DATA.features['User-ID']))}\n",
    "print('all user id = ', all_user)\n",
    "# location的数量=7573(从0开始的)\n",
    "all_location = max([j for i in origin_DATA.features.Location for j in i]) +1 \n",
    "print('all location = ', all_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all isbn =  38036\n",
      "all author =  15196\n",
      "all year =  81\n",
      "all publisher =  2909\n",
      "all title words =  23815\n",
      "all blurb words =  159656\n"
     ]
    }
   ],
   "source": [
    "# ISBN总数\n",
    "all_isbn = len(set(origin_DATA.features['ISBN']))\n",
    "print('all isbn = ', all_isbn)\n",
    "# author总数\n",
    "all_author = len(set(origin_DATA.features['Author']))\n",
    "print('all author = ', all_author)\n",
    "# year总数\n",
    "all_year = len(set(origin_DATA.features['Year']))\n",
    "print('all year = ', all_year)\n",
    "# publish总数\n",
    "all_publisher = len(set(origin_DATA.features['Publisher']))\n",
    "print('all publisher = ', all_publisher)\n",
    "# title中所有单词总数\n",
    "all_title_words = max([j for i in origin_DATA.features.Title for j in i]) +1\n",
    "print('all title words = ', all_title_words)\n",
    "# blurb中所有单词总数\n",
    "all_blurb_words = len(blurb_dict)\n",
    "print('all blurb words = ', all_blurb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_title(all_title_words=all_title_words):\n",
    "    # 将title转换为数字矩阵，并处理长度\n",
    "    # 对title进行补齐\n",
    "    title = []\n",
    "    for ti in origin_DATA.features['Title'].values:\n",
    "        if len(ti) > 10:\n",
    "            ti = ti[:10]\n",
    "        if len(ti) < 10:\n",
    "            ti = ti + [all_title_words] * (10 - len(ti))\n",
    "\n",
    "        title.append(ti)\n",
    "    all_title_words += 1\n",
    "    title = np.array(title)\n",
    "    return title, all_title_words\n",
    "\n",
    "title,all_title_words = pre_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test(): \n",
    "    m = len(origin_DATA.features['Location'])\n",
    "    # 对location取3位数\n",
    "    loca = np.zeros((m, 3))\n",
    "    for i in range(m):\n",
    "        loca[i] = np.array(origin_DATA.features['Location'][i])\n",
    "\n",
    "    input_features = [origin_DATA.features['User-ID'].to_numpy(), loca, \n",
    "                      origin_DATA.features['ISBN'].to_numpy(), origin_DATA.features['Author'].to_numpy(),\n",
    "                     origin_DATA.features['Year'].to_numpy(), origin_DATA.features['Publisher'].to_numpy(), \n",
    "                     title, blurb]\n",
    "    labels = origin_DATA.labels.to_numpy()\n",
    "    # 分割数据集以及shuffle\n",
    "    np.random.seed(100)\n",
    "    number_features = len(input_features)\n",
    "    shuffle_index = np.random.permutation(m)\n",
    "    shuffle_train_index = shuffle_index[:math.ceil(m*0.96)]\n",
    "    shuffle_val_index = shuffle_index[math.ceil(m*0.96): math.ceil(m*0.98)]\n",
    "    shuffle_test_index = shuffle_index[math.ceil(m*0.98):]\n",
    "    train_features = [input_features[i][shuffle_train_index] for i in range(number_features)]\n",
    "    train_labels = labels[shuffle_train_index]\n",
    "    val_features = [input_features[i][shuffle_val_index] for i in range(number_features)]\n",
    "    val_lables = labels[shuffle_val_index]\n",
    "    test_features = [input_features[i][shuffle_test_index] for i in range(number_features)]\n",
    "    test_lables = labels[shuffle_test_index]\n",
    "    return train_features, train_labels, val_features, val_lables, test_features, test_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165218,)\n",
      "(3442,)\n",
      "(3442,)\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, val_features, val_lables, test_features, test_lables = get_train_val_test()\n",
    "print(train_features[0].shape)\n",
    "print(val_features[0].shape)\n",
    "print(test_features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    # 用户特征输入\n",
    "    user_id = keras.layers.Input(shape=(1,), dtype='int32', name='user_id_input')\n",
    "    user_location = keras.layers.Input(shape=(3,), dtype='int32', name='user_location_input')\n",
    "    \n",
    "    # 书籍特征输入\n",
    "    book_isbn = keras.layers.Input(shape=(1,),  dtype='int32', name='book_isbn_input')\n",
    "    book_author = keras.layers.Input(shape=(1,),  dtype='int32', name='book_author_input')\n",
    "    book_year = keras.layers.Input(shape=(1,),  dtype='int32', name='book_year_input')\n",
    "    book_publisher = keras.layers.Input(shape=(1,),  dtype='int32', name='book_publisher_input')  \n",
    "    book_title = keras.layers.Input(shape=(10, ), dtype='int32', name='book_title_input')\n",
    "    book_blurb = keras.layers.Input(shape=(200, ), dtype='int32', name='book_blurb_input')\n",
    "    return user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入矩阵的维度\n",
    "embed_dim = 4\n",
    "embed_dim_title = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embed_layer(u_id, u_loca):\n",
    "    user_id_embedd = keras.layers.Embedding(all_user, embed_dim, name='user_id_embedding')(u_id)\n",
    "    user_loca_embedd = keras.layers.Embedding(all_location, embed_dim , name='user_loca_embedding')(u_loca)\n",
    "    return user_id_embedd, user_loca_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_emded_layer(b_isbn, b_atuhor, b_year, b_publisher, b_title, b_blurb):\n",
    "    book_isbn_embedd = keras.layers.Embedding(all_isbn, embed_dim, name='book_isbn_embedding')(b_isbn)\n",
    "    book_author_embedd = keras.layers.Embedding(all_author, embed_dim, name='book_author_embedding')(b_atuhor)\n",
    "    book_year_embedd = keras.layers.Embedding(all_year, embed_dim, name='book_year_embedding')(b_year)\n",
    "    book_publisher_embedd = keras.layers.Embedding(all_publisher, embed_dim, name='book_publisher_embedding')(b_publisher)\n",
    "    \n",
    "    book_title_embedd = keras.layers.Embedding(all_title_words, embed_dim_title, name='book_title_embedding')(b_title)\n",
    "    # 加载预训练模型\n",
    "    book_blurb_embedd = keras.layers.Embedding(all_blurb_words, blurb_embedding_dim, name='book_blurb_embedding', weights=[blurb_word2vec_martics], trainable=True)(b_blurb)\n",
    "    return book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature(u_id_embedd, u_loca_embedd):\n",
    "    u_id_layer = keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='u_id_dense')(u_id_embedd)\n",
    "#     u_id_layer_drop = keras.layers.Dropout(rate=0.5, name='u_id_layer_drop')(u_id_layer)\n",
    "    # u_id_layer.shape = (?, 1, 32)\n",
    "    # u_loca_layer.shape = (?, 32)\n",
    "    # 这里可以再加个Dense\n",
    "    u_loca_layer = keras.layers.Bidirectional(tf.keras.layers.LSTM(16, name='u_loca_bilstm'), merge_mode='concat')(u_loca_embedd)\n",
    "    u_loca_lstm_dense = keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='u_loca_lstm_dense')(u_loca_layer)\n",
    "    u_id_reshape = keras.layers.Reshape([32])(u_id_layer)\n",
    "    u_combine = keras.layers.concatenate([u_id_reshape, u_loca_lstm_dense],axis=1, name='u_combine')\n",
    "    print(u_combine.shape)\n",
    "    # 这里能不能用激活函数\n",
    "    u_feature_layer = keras.layers.Dense(100, activation='tanh', name='u_feature_layer')(u_combine)\n",
    "    print(u_feature_layer.shape)\n",
    "    return u_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_dense = 4\n",
    "def get_book_feature(b_isbn_embedd, b_author_embedd, b_year_embedd, b_publisher_embedd, b_title_embedd, b_blurb_embedd):\n",
    "    # 首先对前4个特征连接Dense层\n",
    "    b_isbn_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_isbn_dense')(b_isbn_embedd)\n",
    "    b_author_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_author_dense')(b_author_embedd)\n",
    "    b_year_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_year_dense')(b_year_embedd)\n",
    "    b_publisher_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_publisher_dense')(b_publisher_embedd)\n",
    "    # 合并这四个特征,  b_combine_four shape = (?, 1, 16)\n",
    "    b_combine_four = keras.layers.concatenate([b_isbn_dense, b_author_dense, b_year_dense, b_publisher_dense], name='b_four_combine')\n",
    "    print('b_combine_four.shape', b_combine_four.shape)\n",
    "    # 对title进行卷积\n",
    "    b_title_reshape = keras.layers.Lambda(lambda layer: tf.expand_dims(layer, 3))(b_title_embedd)  # shape=(?,10, 16, 1)\n",
    "    print('b_title_reshape.shape = ', b_title_reshape.shape)\n",
    "    # b_title_conv.shape = \n",
    "    b_title_conv = keras.layers.Conv2D(filters=8, kernel_size=(2, embed_dim_title), kernel_regularizer=tf.nn.l2_loss, strides=1)(b_title_reshape)# shape=(?, 14, 1, 8)\n",
    "    # b-title_pool.shape =\n",
    "    b_title_pool = keras.layers.MaxPool2D(pool_size=(9, 1), strides=1)(b_title_conv) # shape=(?,1, 1, 8)\n",
    "    print('b_title_conv.shape = ', b_title_conv)\n",
    "    print('b_title_pool.shape = ', b_title_pool)\n",
    "    \n",
    "    # 对blurb进行处理\n",
    "    # shape = \n",
    "    b_blurb_lstm_1 = keras.layers.Bidirectional(tf.keras.layers.LSTM(16, name='b_blurb_bilstm', dropout=0.5, return_sequences=True), merge_mode='concat')(b_blurb_embedd) \n",
    "    print('b_blurb_lstm_1.shape = ', b_blurb_lstm_1.shape)\n",
    "    b_blurb_lstm_2 = keras.layers.LSTM(32, name='b_blurb_lstm', dropout=0.5, return_sequences=False)(b_blurb_lstm_1) \n",
    "    print('b_blurb_lstm_2.shape = ', b_blurb_lstm_2.shape)\n",
    "    # 将title和blurb合并\n",
    "    b_title_reshape = keras.layers.Reshape([b_title_pool.shape[3]])(b_title_pool)\n",
    "    # b_combine_blurb_title.shape = \n",
    "    b_combine_blurb_title = keras.layers.concatenate([b_title_reshape, b_blurb_lstm_2], axis=1, name='b_combine_blurb_title')\n",
    "    print('b_combine_blurb_title.shape = ', b_combine_blurb_title)\n",
    "    b_blurb_title_dense = keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu', name='b_blurb_title_dense')(b_combine_blurb_title)\n",
    "#     b_blurb_title_dense_drop = keras.layers.Dropout(rate=0.5, name='b_blurb_title_dense_drop')(b_blurb_title_dense)\n",
    "    b_blurb_title_dense = keras.layers.Dense(64, activation='relu', name='b_blurb_title_dense')(b_combine_blurb_title)\n",
    "    # b_combine_four_reshape shape = (?, 64)\n",
    "    b_combine_four_reshape = keras.layers.Reshape([b_combine_four.shape[2]], name='b_combine_four_reshape')(b_combine_four)\n",
    "    # 合并所有的书籍特征\n",
    "#     b_combine_book = keras.layers.concatenate([b_combine_blurb_title, b_combine_four_reshape], axis=1, name='b_combine_book')\n",
    "    b_combine_book = keras.layers.concatenate([b_blurb_title_dense, b_combine_four_reshape], axis=1, name='b_combine_book')\n",
    "\n",
    "    # 得到书籍矩阵\n",
    "    b_feature_layer = keras.layers.Dense(100, name='b_feature_layer', activation='tanh')(b_combine_book)\n",
    "    return b_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(user_feature, book_feature):\n",
    "#     multiply_layer = keras.layers.Lambda(lambda layer: tf.reduce_sum(layer[0]+layer[1], axis=1, keepdims=True), name = 'user_book_feature')((user_feature, book_feature))\n",
    "    inference_layer = keras.layers.concatenate([user_feature, book_feature], axis=1, name='user_book_feature')\n",
    "    inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(inference_layer)\n",
    "    multiply_layer = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "    print(multiply_layer.shape)\n",
    "    return multiply_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "\n",
    "class model_network():\n",
    "    def __init__(self):\n",
    "        self.batchsize = 512\n",
    "        self.epoch = 15\n",
    "    def creat_model(self):\n",
    "        user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb = get_inputs()\n",
    "        user_id_embedd, user_loca_embedd = user_embed_layer(user_id, user_location)\n",
    "        book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd = book_emded_layer(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb)\n",
    "        u_feature_layer = get_user_feature(user_id_embedd, user_loca_embedd)\n",
    "        b_feature_layer = get_book_feature(book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd)\n",
    "        multiply_layer = get_rating(u_feature_layer, b_feature_layer)\n",
    "        model = keras.Model(inputs=[user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb],\n",
    "                    outputs=[multiply_layer])\n",
    "        return model\n",
    "    def train_model(self):\n",
    "        weights_path = './model_weights/model_2.hdf5'\n",
    "#         checkpoint = keras.callbacks.ModelCheckpoint(filepath=weights_path, monitor='val_loss', mode='min', save_weights_only=True)\n",
    "        model_optimizer = keras.optimizers.Adamax()\n",
    "        model = self.creat_model()\n",
    "        model.compile(optimizer=model_optimizer, loss=keras.losses.mse)\n",
    "        history = model.fit(train_features, train_labels, validation_data=(val_features, val_lables), epochs=self.epoch, batch_size=self.batchsize, verbose=1)\n",
    "        print(model.summary())\n",
    "        return model, history\n",
    "    def predict_model(self, model):\n",
    "        test_loss = model.evaluate(test_features, test_lables, verbose=0)\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64)\n",
      "(None, 100)\n",
      "b_combine_four.shape (None, 1, 16)\n",
      "b_title_reshape.shape =  (None, 10, 16, 1)\n",
      "b_title_conv.shape =  Tensor(\"conv2d/Identity:0\", shape=(None, 9, 1, 8), dtype=float32)\n",
      "b_title_pool.shape =  Tensor(\"max_pooling2d/Identity:0\", shape=(None, 1, 1, 8), dtype=float32)\n",
      "b_blurb_lstm_1.shape =  (None, 200, 32)\n",
      "b_blurb_lstm_2.shape =  (None, 32)\n",
      "b_combine_blurb_title.shape =  Tensor(\"b_combine_blurb_title/Identity:0\", shape=(None, 40), dtype=float32)\n",
      "(None, 1)\n",
      "Train on 165218 samples, validate on 3442 samples\n",
      "Epoch 1/15\n",
      "165218/165218 [==============================] - 43s 261us/sample - loss: 26.9403 - val_loss: 17.4714\n",
      "Epoch 2/15\n",
      "165218/165218 [==============================] - 33s 197us/sample - loss: 15.4060 - val_loss: 13.3318\n",
      "Epoch 3/15\n",
      "165218/165218 [==============================] - 33s 199us/sample - loss: 13.2912 - val_loss: 12.4378\n",
      "Epoch 4/15\n",
      "165218/165218 [==============================] - 33s 200us/sample - loss: 12.7005 - val_loss: 12.1677\n",
      "Epoch 5/15\n",
      "103424/165218 [=================>............] - ETA: 12s - loss: 12.4615- ETA: 16s - lo - ETA: 12s - loss: 12. - ETA: 12s - loss: 12.4623"
     ]
    }
   ],
   "source": [
    "net_work = model_network()\n",
    "model, history = net_work.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(1)\n",
    "plt.plot(train_loss, c='r', label='train_loss')\n",
    "plt.plot(val_loss, c='b', label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlim([0, 15])\n",
    "# plt.ylim([0, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = net_work.predict_model(model)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画出模型图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, to_file='model_2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
